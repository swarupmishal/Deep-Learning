dataset = GermanCredit
str(dataset)
dataset[,1:7] = as.data.frame(lapply(dataset[,1:7], scale))
str(dataset)
dataset
sample_index = sample(1000, 200)
test_dateset = dataset[sample_index,]
train_dateset = dataset[-sample_index,]
x<-subset(train_dateset, select = -Class)
y<- train_dateset$Class
model<-svm(x,y, scale=F)
obj<-tune(svm, Class~.,kernel ="radial", data= train_dateset, ranges=list(gamma=2^(-15:3), cost=2^(-5:15)))
i: ```#install.packages("caret")
#install.packages("e1701")
library(caret)
library(e1071)
data(GermanCredit)
dataset = GermanCredit
str(dataset)
dataset[,1:7] = as.data.frame(lapply(dataset[,1:7], scale))
str(dataset)
dataset
sample_index = sample(1000, 200)
test_dateset = dataset[sample_index,]
train_dateset = dataset[-sample_index,]
x<-subset(train_dateset, select = -Class)
y<- train_dateset$Class
model<-svm(x,y, scale=F)
obj<-tune(svm, Class~.,kernel ="radial", data= train_dateset, ranges=list(gamma=2^(-15:3), cost=2^(-5:15)))
obj<-tune(svm, Class~.,kernel ="radial", data= train_dateset, ranges=list(gamma=10^(-5:1), cost=10^(-3:5)))
obj<-tune(svm, Class~.,kernel ="radial", data= train_dateset, ranges=list(gamma=10^(-1:1), cost=10^(-1:1)))
obj<-tune(svm, Class~.,kernel ="radial", data= train_dateset, ranges=list(gamma=10^(-1:1), cost=10^(-1:1)))
summary(obj)
model = svm(Class ~ ., kernel = "radial", cost = 1, gamma = 0.1, data = train_dateset, scale = F)
predictions <-  predict(model, test_dateset[-10])
table(test_dateset[,10], predictions)
install.packages("caret")
library(caret)
library(e1071)
data(GermanCredit)
dataset = GermanCredit
str(dataset)
dataset[,1:7] = as.data.frame(lapply(dataset[,1:7], scale))
write.csv(dataset, file = "GermanCredit.csv")
sample_index = sample(1000, 200)
test_dataset = dataset[sample_index,]
train_dataset = dataset[-sample_index,]
obj<-tune(svm, Class~.,kernel ="radial", data= train_dateset, ranges=list(gamma=10^(-1:1), cost=10^(-1:1)))
summary(obj)
obj<-tune(svm, Class~.,kernel ="linear", data= train_dateset, ranges=list(gamma=10^(-1:1), cost=10^(-1:1)))
summary(obj)
obj<-tune(svm, Class~.,kernel ="polynomial", data= train_dateset, ranges=list(gamma=10^(-1:1), cost=10^(-1:1)))
summary(obj)
obj<-tune(svm, Class~.,kernel ="sigmoid", data= train_dateset, ranges=list(gamma=10^(-1:1), cost=10^(-1:1)))
summary(obj)
library(caret)
library(e1071)
data(GermanCredit)
dataset = GermanCredit
str(dataset)
dataset[,1:7] = as.data.frame(lapply(dataset[,1:7], scale))
str(dataset)
sample_index = sample(1000, 200)
test_dataset = dataset[sample_index,]
train_dataset = dataset[-sample_index,]
obj<-tune(svm, Class~.,kernel ="radial", data= train_dateset, ranges=list(gamma=10^(-2:2), cost=10^(-2:2)))
summary(obj)
obj<-tune(svm, Class~.,kernel ="linear", data= train_dateset, ranges=list(gamma=10^(-2:2), cost=10^(-2:2)))
summary(obj)
obj<-tune(svm, Class~.,kernel ="polynomial", data= train_dateset, ranges=list(gamma=10^(-2:2), cost=10^(-2:2)))
summary(obj)
obj<-tune(svm, Class~.,kernel ="sigmoid", data= train_dateset, ranges=list(gamma=10^(-2:2), cost=10^(-2:2)))
summary(obj)
library(caret)
library(e1071)
data(GermanCredit)
dataset = GermanCredit
str(dataset)
dataset[,1:7] = as.data.frame(lapply(dataset[,1:7], scale))
str(dataset)
sample_index = sample(1000, 200)
test_dataset = dataset[sample_index,]
train_dataset = dataset[-sample_index,]
model = svm(Class ~ ., kernel = "radial", cost = 10, gamma = 0.1, data = train_dataset, scale = F)
predictions <-  predict(model, test_dataset[-10])
table(test_dateset[,10], predictions)
model = svm(Class ~ ., kernel = "radial", cost = 10, gamma = 0.1, data = train_dataset, scale = F)
predictions <-  predict(model, test_dataset[-10])
table(test_dateset[,10], predictions)
print(summary(relation))
install.packages(party)
install.packages("party")
x <- c(151, 174, 138, 186, 128, 136, 179, 163, 152, 131)
y <- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)
relation <- lm(y~x)
print(summary(relation))
print(result)
a <- data.frame(x=170)
result <- predict(relation,a)
print(result)
input <- mtcars[,c("am","cyl","hp","wt")]
print(head(input))
am.data = glm(formula = am ~ cyl + hp + wt, data = input, family = binomial)
print(summary(am.data))
library(party)
input.dat <- readingSkills[c(1:105),]
png(file = "decision_tree.png")
output.tree <- ctree(
nativeSpeaker ~ age + shoeSize + score,
data = input.dat)
plot(output.tree)
dev.off()
dev.off()
plot(output.tree)
library(party)
library(randomForest)
output.forest <- randomForest(nativeSpeaker ~ age + shoeSize + score,
data = readingSkills)
print(output.forest)
print(importance(fit,type = 2))
?contrasts
install.packages("XLConnect")
library(XLConnect)
loan <- readWorksheetFromFile("D:/Spring 2017/ADS/loan.xlsx", sheet=1)
loan
Res_status <- as.factor(loan$Res_status)
occupation <- as.factor(loan$Occupation)
Job_Status <- as.factor(loan$Job_status)
Liab_ref <- as.factor(loan$Liab_ref)
Acc_ref <- as.factor(loan$Acc_ref)
Decision <- as.factor(loan$Decision)
library(ISLR)
print(head(College,2))
install.packages('ISLR')
library(ISLR)
print(head(College,2))
maxs <- apply(College[,2:18], 2, max)
mins <- apply(College[,2:18], 2, min)
scaled.data <- as.data.frame(scale(College[,2:18],center = mins, scale = maxs - mins))
print(head(scaled.data,2))
Private = as.numeric(College$Private)-1
library(neuralnet)
data = cbind(Private,scaled.data)
library(caTools)
set.seed(101)
split = sample.split(data$Private, SplitRatio = 0.70)
train = subset(data, split == TRUE)
test = subset(data, split == FALSE)
feats <- names(scaled.data)
f <- paste(feats,collapse=' + ')
f <- paste('Private ~',f)
f <- as.formula(f)
f
library(neuralnet)
nn <- neuralnet(f,data,hidden=c(10,10,10),linear.output=FALSE)
install.packages('neuralnet')
library(neuralnet)
nn <- neuralnet(f,data,hidden=c(10,10,10),linear.output=FALSE)
predicted.nn.values <- compute(nn,test[2:18])
print(head(predicted.nn.values$net.result))
predicted.nn.values$net.result <- sapply(predicted.nn.values$net.result,round,digits=0)
table(test$Private,predicted.nn.values$net.result)
library("NLP")
install.packages("NLP")
library("NLP")
library("openNLP")
install.packages("openNLP")
library("NLP")
library("openNLP")
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ",
"nonexecutive director Nov, 29.\n",
"Mr. Vinken is chairman of Elsevier N.V., ",
"the Dutch publishing group."), collapse = "")
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
post_tag_annotator <- Maxent_POS_Tag_Annotator()
post_tag_annotator
a3 <- annotate(s, post_tag_annotator, a2)
a3
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
sprintf("%s/%s", s[a3w], tags)
a3ws2 <- annotations_in_spans(subset(a3, type == word"")
subset(a3, type == "sentence")[2L])[[1L]]
a3ws2 <- annotations_in_spans(subset(a3, type == word"")
subset(a3, type == "sentence")[2L])[[1L]]
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
post_tag_annotator <- Maxent_POS_Tag_Annotator()
post_tag_annotator
a3 <- annotate(s, post_tag_annotator, a2)
a3
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
sprintf("%s/%s", s[a3w], tags)
a3ws2 <- annotations_in_spans(subset(a3, type == word"")
subset(a3, type == "sentence")[2L])[[1L]]
a3ws2 <- annotations_in_spans(subset(a3, type == "word")subset(a3, type == "sentence")[2L])[[1L]]
a3ws2 <- annotations_in_spans(subset(a3, type == "word"),
subset(a3, type == "sentence")[2L])[[1L]]
sprintf("%s/%s", s[a3w3s], sapply(a3ws2$features, `[[`, "POS"))
sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
library(XLConnect)
library("neuralnet")
loan=read.xlsx("loan.xlsx",sheetIndex = 1,HEADER=TRUE)
library("XLConnect")
install.packages("rvest")
library(rvest)
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
View(all_reviews)
install.packages("rJava")
library(rJava)
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
install.packages(c("NLP", "openNLP", "RWeka", "qdap"))
install.packages(c("NLP", "openNLP", "RWeka", "qdap"))
library(NLP)
library(openNLP)
library(RWeka)
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
song <- paste0("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
tokenize_words(song)
library(stringi)
library(stringi)
library(SnowballC)
install.packages("SnowballC")
library(stringi)
library(SnowballC)
library(Rcpp)
song <- paste0("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
tokenize_words(song)
install.packages("Rstem")
install.packages("tm")
install.packages("purrr")
library(stringi)
library(Rstem)
library(tm)
library(purrr)
song <- paste0("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
tokenize_words(song)
library(dplyr)
song <- paste0("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
tokenize_words(song)
Token_Tokenizer(song)
song1 <- Token_Tokenizer(song)
View(song1)
song <- paste("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
songtokenize_words(song)
song
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
token(all_reviews)
tokenized_reviews <- token(all_reviews)
wordStem(tokenized_reviews)
View(all_reviews)
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
tokenized_reviews <- token(all_reviews)
wordStem(tokenized_reviews)
library(rvest)
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
tokenized_reviews <- token(all_reviews)
wordStem(tokenized_reviews)
install.packages("RTextTools")
library(RTextTools)
pages <- 100
all_reviews <- NULL
View(all_reviews)
pages <- 10
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
tokenized_reviews <- token(all_reviews)
wordStem(tokenized_reviews)
library(deepnet)
install.packages("deepnet")
library(deepnet)
show.digit <- function(arr784, col=gray(12:1/12), ...) {
image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
library(deepnet)
show.digit <- function(arr784, col=gray(12:1/12), ...) {
image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
setwd("D:/Spring 2017/ADS/Final Project/MNIST")
mnist<-load.mnist(".")
show.digit(mnist$train$x[155,])
dnn <- dbn.dnn.train(mnist$train$x, mnist$train$yy, hidden = c(300,300), numepochs = 2, cd=2)
err.dnn <- nn.test(dnn, mnist$test$x, mnist$test$yy)
yy.dnn <- nn.predict(dnn, mnist$test$x)
show.digit(mnist$test$x[99,])
Var1 <- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
Var2 <- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
x <- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
y <- c(rep(1, 50), rep(0, 50))
test_Var1 <- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
test_Var2 <- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
test_x <- matrix(c(test_Var1, test_Var2), nrow = 100, ncol = 2)
dnn.small <- dbn.dnn.train(x, y, hidden = c(5, 5), numepochs=5)
err.dnn.small <- nn.test(dnn.small, test_x, y)
yy.dnn.small <- nn.predict(dnn.small, test_x)
yy.dnn.small
require(mlbench)
data(Vehicle)
require(Rdbn)
?data
library(deepnet)
setwd("D:/Spring 2017/ADS/Final Project")
final <-read.csv(file="final.csv", header=TRUE, sep=",")
show.digit <- function(arr784, col=gray(12:1/12), ...) {
image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
mnist<-load.mnist(".")
setwd("D:/Spring 2017/ADS/Final Project/MNIST")
mnist<-load.mnist(".")
show.digit(mnist$train$x[155,])
View(mnist$train$x)
View(mnist$train$yy)
View(mnist$test$yy)
ncol(final)
test <- final[4001:5000]
final <-read.csv(file="final.csv", header=TRUE, sep=",")
setwd("D:/Spring 2017/ADS/Final Project")
final <-read.csv(file="final.csv", header=TRUE, sep=",")
train <- final[1:4000]
test <- final[4001:5000]
setwd("D:/Spring 2017/ADS/Final Project")
final <-read.csv(file="final.csv", header=TRUE, sep=",")
train <- final[1:4000]
test <- final[4001:5000]
ncol(final)
train <- final[1:4000,]
test <- final[4001:5000,]
dnn <- dbn.dnn.train(train[,1:6177], train$RESULT, hidden = c(300,300), numepochs = 2, cd=2)
dnn <- dbn.dnn.train(train[,1:6177], train[,6178], hidden = c(300,300), numepochs = 2, cd=2)
library(darch)
setwd("D:/Spring 2017/ADS/Final Project")
final <-read.csv(file="final_temp.csv", header=TRUE, sep=",")
train<-final[1:4000,]
test<-final[4001:5000,]
ncol(train)
darch  <- darch(train[,1:1377], train$RESULT, rbm.numEpochs = 10, rbm.batchSize = 100, rbm.trainOutputLayer = F, layers = c(75,100,10), darch.batchSize = 100, darch.learnRate = 2, darch.retainData = F, darch.numEpochs = 20 )
darch  <- darch(train[,1:1377], train$RESULT, final, rbm.numEpochs = 10, rbm.batchSize = 100, rbm.trainOutputLayer = F, layers = c(75,100,10), darch.batchSize = 100, darch.learnRate = 2, darch.retainData = F, darch.numEpochs = 20 )
darch  <- darch(train[,1:1377], train$RESULT, final, rbm.numEpochs = 10, rbm.batchSize = 100, rbm.trainOutputLayer = F, layers = c(75,100,10), darch.batchSize = 100, darch.learnRate = 2, darch.retainData = F, darch.numEpochs = 20 )
darch  <- darch(train[,1:1377], train$RESULT, train, rbm.numEpochs = 10, rbm.batchSize = 100, rbm.trainOutputLayer = F, layers = c(75,100,10), darch.batchSize = 100, darch.learnRate = 2, darch.retainData = F, darch.numEpochs = 20 )
View(final)
library(tm)
library(RTextTools)
library(rvest)
library(dplyr)
library(plyr)
library(stringr)
library(e1071)
library(forecast)
library(ade4)
library(ggplot2)
library("rpart")
library("rpart.plot")
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
